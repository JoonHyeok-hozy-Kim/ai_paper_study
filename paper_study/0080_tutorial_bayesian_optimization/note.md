* [Back to Main](../../README.md)
---

# A Tutorial on Bayesian Optimization
### Peter I. Frazier
* [Read Paper](../paper_pdfs/240827%20A%20Tutorial%20on%20Bayesian%20Optimization.pdf)

---

## 1. Introduction
### Concept) Bayesian Optimization (BayesOpt)
- Def.)
  - A class of machine-learning-based optimization methods focused on solving the problem
    - $`\displaystyle\max_{x\in A} f(x)`$
      - where the feasible set $`A`$ and objective function $`f`$ typically have the following properties:
        - The input $`x\in \mathbb{R}^d`$
          - $`d`$ is not too large
          - Typically, $`d\le 20`$ in most successful applications of BayesOpt.
        - The feasible set $`A`$ is a simple set, in which it is easy to assess membership.
          - Typically $`A`$ is a hyper-rectangle $`\{x\in\mathbb{R}^d : a_i \le x_i \le b_i\}`$ or the $`d\textrm{-dimensional simplex } \{x\in\mathbb{R}^d : \sum_i x_i = 1\}`$
        - The objective function $`f`$ is continuous.
          - This will typically be required to model $`f`$ using Gaussian process regression.
        - $`f`$ is “expensive to evaluate”
          - i.e.) Each evaluation takes a substantial amount of time or monetary/opportunity cost
        - $`f`$ is a “black box.”
          - i.e.) $`f`$ lacks known special structure like concavity or linearity that would make it easy to optimize using techniques that leverage such structure to improve efficiency.
        - Derivative Free
          - When evaluating $`f`$, we observe only $`f(x)`$.
            - No first- or second-order derivatives are available.
              - Thus, we **cannot** use
                - gradient descent
                - Newton's methods
                - quasi-Newton methods
        - Assume that $`f(x)`$ is observed **WITHOUT** noise.
        - Our goal is to find a **global** rather than local optimum.

<br>

## 2. Overview of BayesOpt
### Algorithm 1) Basic pseudo-cod for Bayesian optimization
- Algorithm)
  - Place a Gaussian process prior on $`f`$
  - Observe $`f`$ at $`n_0`$ points according to an initial space-filling experimental design.
  - Set $`n = n_0`$
  - `while` $`n\le N`$ `do`
    - Update the posterior probability distribution on $`f`$ using all available data.
    - Let $`x_n`$ be a maximizer of the acquisition function over $`x`$,
      - where the acquisition function is computed using the current posterior distribution.
    - Observe $`y_n = f(x_n)`$.
    - Increment $`n`$.
  - `end while`
  - Return a solution:
    - Either
      - the point evaluated with the largest $`f(x)`$
      - the the point with the largest posterior mean.
- Desc.)
  - The algorithm consists of two main components:
    1. A Bayesian [statistical model](#concept-statistical-model) for modeling the **objective function**
       - How?)
         - Evaluate the objective function according to an initial space-filling experimental design. 
         - Points are often chosen uniformly at random.
         - Points are used iteratively to allocate the remainder of a budget of $`N`$ function evaluations.
    2. An **acquisition function** for deciding where to sample next

<br>

#### Concept) Statistical Model
- Desc.)
  - Invariably a Gaussian process
  - It provides a Bayesian posterior probability distribution that describes potential values for $`f(x)`$ at a candidate point $`x`$.
  - Each time we observe $`f`$ at a new point, the posterior distribution is updated.

#### Concept) Acquisition Function
- Desc.)
  - It measures the value that would be generated by evaluation of the objective function at a new point $`x`$
    - based on the current posterior distribution over $`f`$.

#### e.g.) One Iteration using GP regression and Expected Improvements
||
|:-|
|![](images/001.png)|
|- blue circles : noise-free observations of the objective functions <br> - GP regression produces a posterior probability on each $`f(x) \sim N(\mu_n(x), \sigma^2_n(x))`$ <br> - solid red line : $`\mu_n(x)`$ <br> - dashed red lines : a 95% Bayesian credible interval for $`f(x)`$, i.e. $`\mu_n(x)\pm 1.96\times\sigma_n(x)`$|
|![](images/002.png)|
|- The expected improvement acquisition function that corresponds to this posterior. <br> - Value 0 at points that have previously been evaluated.|

<br>

## 3. Gaussian Process (GP) Regression



---
* [Back to Main](../../README.md)