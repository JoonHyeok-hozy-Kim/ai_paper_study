* [Back to Main](../../README.md)
---

# A Tutorial on Bayesian Optimization
### Peter I. Frazier
* [Read Paper](../paper_pdfs/240827%20A%20Tutorial%20on%20Bayesian%20Optimization.pdf)

---

## 1. Introduction
### Concept) Bayesian Optimization (BayesOpt)
- Def.)
  - A class of machine-learning-based optimization methods focused on solving the problem
    - $`\displaystyle\max_{x\in A} f(x)`$
      - where the feasible set $`A`$ and objective function $`f`$ typically have the following properties:
        - The input $`x\in \mathbb{R}^d`$
          - $`d`$ is not too large
          - Typically, $`d\le 20`$ in most successful applications of BayesOpt.
        - The feasible set $`A`$ is a simple set, in which it is easy to assess membership.
          - Typically $`A`$ is a hyper-rectangle $`\{x\in\mathbb{R}^d : a_i \le x_i \le b_i\}`$ or the $`d\textrm{-dimensional simplex } \{x\in\mathbb{R}^d : \sum_i x_i = 1\}`$
        - The objective function $`f`$ is continuous.
          - This will typically be required to model $`f`$ using Gaussian process regression.
        - $`f`$ is “expensive to evaluate”
          - i.e.) Each evaluation takes a substantial amount of time or monetary/opportunity cost
        - $`f`$ is a “black box.”
          - i.e.) $`f`$ lacks known special structure like concavity or linearity that would make it easy to optimize using techniques that leverage such structure to improve efficiency.
        - Derivative Free
          - When evaluating $`f`$, we observe only $`f(x)`$.
            - No first- or second-order derivatives are available.
              - Thus, we **cannot** use
                - gradient descent
                - Newton's methods
                - quasi-Newton methods
        - Assume that $`f(x)`$ is observed **WITHOUT** noise.
        - Our goal is to find a **global** rather than local optimum.

<br>

## 2. Overview of BayesOpt
### Algorithm 1) Basic pseudo-code for Bayesian optimization
- Algorithm)
  - Place a Gaussian process prior on $`f`$
  - Observe $`f`$ at $`n_0`$ points according to an initial space-filling experimental design.
  - Set $`n = n_0`$
  - `while` $`n\le N`$ `do`
    - Update the posterior probability distribution on $`f`$ using all available data.
    - Let $`x_n`$ be a maximizer of the acquisition function over $`x`$,
      - where the acquisition function is computed using the current posterior distribution.
    - Observe $`y_n = f(x_n)`$.
    - Increment $`n`$.
  - `end while`
  - Return a solution:
    - Either
      - the point evaluated with the largest $`f(x)`$
      - the the point with the largest posterior mean.
- Desc.)
  - The algorithm consists of two main components:
    1. A Bayesian [statistical model](#concept-statistical-model) for modeling the **objective function**
       - How?)
         - Evaluate the objective function according to an initial space-filling experimental design. 
         - Points are often chosen uniformly at random.
         - Points are used iteratively to allocate the remainder of a budget of $`N`$ function evaluations.
    2. An **acquisition function** for deciding where to sample next

<br>

#### Concept) Statistical Model
- Desc.)
  - Invariably a Gaussian process
  - It provides a Bayesian posterior probability distribution that describes potential values for $`f(x)`$ at a candidate point $`x`$.
  - Each time we observe $`f`$ at a new point, the posterior distribution is updated.

#### Concept) Acquisition Function
- Desc.)
  - It measures the value that would be generated by evaluation of the objective function at a new point $`x`$
    - based on the current posterior distribution over $`f`$.

#### e.g.) One Iteration using GP regression and Expected Improvements
||
|:-|
|![](images/001.png)|
|- blue circles : noise-free observations of the objective functions <br> - GP regression produces a posterior probability on each $`f(x) \sim N(\mu_n(x), \sigma^2_n(x))`$ <br> - solid red line : $`\mu_n(x)`$ <br> - dashed red lines : a 95% Bayesian credible interval for $`f(x)`$, i.e. $`\mu_n(x)\pm 1.96\times\sigma_n(x)`$|
|![](images/002.png)|
|- The expected improvement acquisition function that corresponds to this posterior. <br> - Value 0 at points that have previously been evaluated.|

<br>

## 3. Gaussian Process (GP) Regression
### Concept) Gaussian Process Regression
- Desc.)
  - A Bayesian statistical approach for **modeling functions**
  - Here, the brief introduction is provided.
    - For more complete treatment, refer to Rasmussen, C. and Williams, C. (2006). *Gaussian Processes for Machine Learning.*
- Model)
  - $`f:\mathbb{R}^d\rightarrow\mathbb{R}`$
    - $`[f(x_1), f(x_2), \cdots, f(x_k)]`$ : $`f`$'s values at a finite collection of points $`x_1, x_2,\cdots, x_k\in\mathbb{R}^d`$ in a vector.
      - Assumptions)
        - They were drawn at random by nature from some **prior probability distribution**.
        - GP Regression takes this prior distribution to be multivariate normal with...
          - **Mean Vector**
            - How to construct?)
              - Evaluate a mean function $`\mu_0`$ at each $`x_i`$.
                - e.g.) Mean Function
                  - d
          - **Covariance Matrix**
            - How to construct?)
              - Evaluation a **covariance function** or **kernel** $`\Sigma_0`$ at each pair of point $`x_i, x_j`$.
                - $`x_i, x_j`$ are closer $`\rightarrow`$ They have a larger positive correlation.
                - The kernel must be chosen to make the covariance matrix be positive semi-definite, regardless of the collection of points chosen.
                - e.g.) Kernel
                  - d
  - The resulting prior distribution on $`[f(x_1), f(x_2), \cdots, f(x_k)]`$
    - $`\displaystyle f(x_{1:k})\sim \textrm{Normal}(\mu_0(x_{1:k}), \Sigma_0(x_{1:k}, x_{1:k}))`$ 
      - where
        - $`x_{1:k}=x_1,x_2,\cdots, x_k`$
        - $`f(x_{1:k})=[f(x_1), f(x_2), \cdots, f(x_k)]`$
        - $`\mu_0(x_{1:k})=[\mu_0(x_1), \mu_0(x_2), \cdots, \mu_0(x_k)]`$
        - $`\Sigma_0(x_{1:k})=[\Sigma_0(x_1, x_1), \Sigma_0(x_1, x_2), \cdots, \Sigma_0(x_1, x_k) ; \cdots ; \Sigma_0(x_k, x_1), \Sigma_0(x_k, x_2), \cdots, \Sigma_0(x_k, x_k)]`$


---
* [Back to Main](../../README.md)