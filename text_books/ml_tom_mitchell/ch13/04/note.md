* [Back to Machine Learning Tom Mitchell Main](../../main.md)

# 13.4 Non-Deterministic Rewards and Actions

#### Assumptions
- Non-Deterministic reward function $r(s,a)$ and action transition function $\delta(s,a)$.
- Adapt probabilities to $r$ and $\delta$.
  - How?)
    - $r$ and $\delta$ may produce a probability distribution function over $s$ and $a$.
    - A.K.A. Non-Deterministic Markov Decision Process

<br>

### Model
#### Objective Function
- Recall that the new reward function and the new action transition function are probabilistic.
- Thus, the objective function $V^\pi$ for the policy $\pi$ should be the expected value of the discounted cumulative rewards.
  - $`\displaystyle V^\pi (s_t) \equiv E\left[ \sum_{i=0}^\infty \gamma^i r_{t+i} \right]`$
    - where $r_{t+i}$ is generated by following policy $\pi$ beginning at state $s_t$

#### Optimization
- Let's derive the optimal policy $\pi^\ast$.




<br>

* [Back to Machine Learning Tom Mitchell Main](../../main.md)