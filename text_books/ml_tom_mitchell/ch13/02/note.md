* [Back to Machine Learning Tom Mitchell Main](../../main.md)

# 13.2 The Learning Task

### Concept) Markov Decision Process (MDP)
- Desc.)
  - In MDP, the **agent** 
    - perceives a set $S$ of distinct **states** of its environment
    - has a set $A$ of **actions** that it can perform.
      - In this chapter, $S$ and $A$ are assumed to be **finite**.
  - At each discrete time step $t$, 
    - the **agent** 
      - senses the current **state** $s_t$
      - chooses a current **action** $a_t$
      - **performs** it.
    - the **environment** responds by
      - giving the agent a **reward** $r_t=r(s_t, a_t)$
      - producing the succeeding **state** $s_{t+1} = \delta(s_t, a_t)$
        - where functions $\delta$ and $r$ are not necessarily known to the agent
          - $\delta$ and $r$ depend only on $s_t$ and $a_t$
          - In this chapter, $\delta$ and $r$ are assumed to be **deterministic**
            - cf.) $\delta$ and $r$ are generally **non-deterministic**.
  - Task of the Agent
    - Learn a policy $\pi : S \rightarrow A$
      - $\pi(s_t) = a_t$
        - i.e.) Choose an action $a_t \in A$ based on $s_t\in S$.
    - How?)
      - Require the policy that produces the **greatest possible cumulative reward** for the agent over time.
        - Put 
          - $`\displaystyle V^\pi(s_t) \equiv r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots = \sum_{i=0}^\infty\gamma^i r_{t+i}`$ : the cumulative value
            - where 
              - the sequence of rewards $r_{t+1}$ is generated by beginning at state $s_t$
              - $0\le\gamma\lt 1$ : a constant that determines the relative value of delayed vs. immediate reward is considered.
                - $\gamma\rightarrow 0$ : The immediate reward is more considered.











<br>

* [Back to Machine Learning Tom Mitchell Main](../../main.md)