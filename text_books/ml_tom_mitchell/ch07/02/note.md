* [Back to Machine Learning Tom Mitchell Main](../../main.md)



#### Concept) Key Questions
1. Sample complexity
   - How many training examples are needed for a learner to converge (with high probability) to a successful hypothesis? 
2. Computational complexity
   - How much computational effort is needed for a learner to converge (with high probability) to a successful hypothesis? 
3. Mistake bound
   - How many training examples will the learner misclassify before converging to a successful hypothesis?

<br><br>

# 7.2 Probably Learning an Approximately Correct Hypothesis (PAC)
## 7.2.1 The Problem Setting
- Assumptions)
  - $X$ : the set of all possible instances over which target functions may be defined
    - $x\in X$ might be described by some attributes.
  - $C$ : the set of target concepts that our learner might be called upon to learn
    - $c\in C$ can be described as $c:X \rightarrow \lbrace 0,1 \rbrace$.
      - i.e.) $` c(x)= \left\lbrace\begin{array}{ll} 1 & \textrm{if }x\textrm{ is a positive example} \\ 0 & \textrm{otherwise.} \end{array}\right.`$
  - Instances are generated from $X$ according to some **probability distribution** $D$.
    - $D$ can be any distribution.
    - Generally, $D$ is not known to the learner.
    - $D$ is assumed to be stationary, i.e. does not change!
  - Training examples are generated by drawing $x$ at random according to $D$ along with its target value $c(x)$.
  - The learner $L$ considers a possible hypothesis space $H$ to learn the target concept $c$.
    - $L$ outputs an hypothesis $h \in H$ which is its estimate of $c$.
    - We evaluate the performance of $h$ over new instances drawn random from $X$ according to $D$.

<br><br>

## 7.2.2 Error of Hypothesis
#### Def.) The True Error of a Hypothesis
- The true error of $h$ w.r.t. $c$ and $D$ is the probability that $h$ will misclassify an instance drawn at random according to $D$.
  - $error_D(h) \equiv Pr_{x\in D}[c(x) \ne h(x)]$ 

![](images/001.png)

<br>

#### Props.) True Error
- $error_D(h)$ depends strongly on the unknown probability distribution $D$.
- $error_D(h)$ is not directly observable to the learner. 
  - $L$ can only observe the performance of $h$ over the training examples, and it must choose its output hypothesis on this basis only.
  - Concept) Training Error
    - the fraction of training examples misclassified by $h$


<br>

* [Back to Machine Learning Tom Mitchell Main](../../main.md)