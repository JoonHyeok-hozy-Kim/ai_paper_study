* [Back to Machine Learning Tom Mitchell Main](../../main.md)

# 6.12 The EM Algorithm
- Desc.)
  - A widely used approach to learning in the presence of **unobserved variables**.
    - It be used even for variables whose value is never directly observed, provided the general form of the **probability distribution governing these variables is known**. 

## 6.12.1 Estimating Means of k Gaussians
- Example Case)
  - Settings)
    - $D$ : a dataset consisted of instances generated by a probability distribution that is a mixture of $k$ distinct Normal distributions.
      - For simplicity, assume the followings.
           - Each Normal distribution has the same variance $\sigma^2$.
           - $\sigma^2$ is known.
    - Assume $k=2$.
    - Repeat the following two-step process to generate an instance $x_i$ that forms $D$.
      1. One of the $k$ Normal distributions is selected at random.
         - For simplicity, assume the followings.
           - Each distribution is selected with uniform probability.
      2. A single random instance $x_i$ is generated according to this selected distribution.
  - Learning Task)
    - Output a hypothesis $h=\langle \mu_1, \cdots, \mu_k \rangle$ that describes the means of each of $k$ distributions.
      - Since $k=2$, $h=\langle \mu_1,\mu_2 \rangle$ in our problem.
    - We will find a maximum likelihood hypothesis.
      - i.e.) $argmax_{d \in D} p(d|h)$
      - Problem)
        - We don't know from which one among $k$ distributions an instance $x_i$ originated.
        - Thus, we will adopt additional variable $z_{ij}$ such that
          - $`z_{ij}=\left\lbrace \begin{array}{ll} 1 & \textrm{if } x_i \textrm{ was created by the }j\textrm{th Normal distribution} \\ 0 & \textrm{otherwise} \end{array} \right.`$












<br>

* [Back to Machine Learning Tom Mitchell Main](../../main.md)