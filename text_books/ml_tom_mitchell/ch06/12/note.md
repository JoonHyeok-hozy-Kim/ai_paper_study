* [Back to Machine Learning Tom Mitchell Main](../../main.md)

# 6.12 The EM Algorithm
- Desc.)
  - A widely used approach to learning in the presence of **unobserved variables**.
    - Can be used even for variables whose value is never directly observed, provided the general form of the **probability distribution governing these variables is known**. 
- The actual algorithm can be found [below](#algorithm-the-em-algorithm).

<br>

## 6.12.1 Estimating Means of k Gaussians
- General case for the $k$ Means Algorithm is described [below](#6123-derivation-of-the-k-means-algorithm).

#### Example Case) k=2
  - Settings)
    - $D$ : a dataset consisted of instances generated by a probability distribution that is a mixture of $k$ distinct Normal distributions.
      - For simplicity, assume the followings.
           - Each Normal distribution has the same variance $\sigma^2$.
           - $\sigma^2$ is known.
    - Assume $k=2$.
    - Repeat the following two-step process to generate an instance $x_i$ that forms $D$.
      1. One of the $k$ Normal distributions is selected at random.
         - For simplicity, assume the followings.
           - Each distribution is selected with uniform probability.
      2. A single random instance $x_i$ is generated according to this selected distribution.
  - Learning Task)
    - Output a hypothesis $h=\langle \mu_1, \cdots, \mu_k \rangle$ that describes the means of each of $k$ distributions.
      - Since $k=2$, $h=\langle \mu_1,\mu_2 \rangle$ in our problem.
    - We will find a maximum likelihood hypothesis.
      - i.e.) $argmax_{d \in D} p(d|h)$
      - Problem)
        - We don't know from which one among $k$ distributions an instance $x_i$ originated.
        - Thus, we will adopt hidden variable $z_{ij}$.
          - Here, $z_{ij}$ denotes the probability that $x_i$ was created by the $j$-th Normal distribution.
          - $`z_{ij}=\left\lbrace \begin{array}{ll} 1 & \textrm{if } x_i \textrm{ was created by the }j\textrm{th Normal distribution} \\ 0 & \textrm{otherwise} \end{array} \right.`$
            - e.g.) 
              - If $x_1$ is originated from the second distribution, $z_{12}=1$.
              - If $x_5$ is not originated from the second distribution, $z_{51}=0$.
  - Solution)
    - Initialize an arbitrary value for $\langle \mu_1,\mu_2 \rangle$.
    - Repeat the following steps.
      1. Calculate the expected value $E[z_{ij}]$ of each hidden variable $z_{ij}$, assuming the current hypothesis $h=\langle \mu_1,\mu_2 \rangle$ holds.
         - How?)
           - Recall that $z_{ij}$ denotes the probability that $x_i$ was created by the $j$-th Normal distribution.   
           - Thus,   
             $`\begin{array}{lll} E[z_{ij}] &= \frac{p(x=x_i|\mu=\mu_j)}{\Sigma_{n \in \lbrace 1, 2\rbrace} p(x=x_i|\mu=\mu_n)} &\\&= \frac{e^{-\frac{1}{2\sigma^2}(x_i-\mu_j)^2}}{\Sigma_{n \in \lbrace 1, 2\rbrace} e^{-\frac{1}{2\sigma^2}(x_i-\mu_n)^2}} & \because \textrm{Normal distribution}\end{array}`$
      2. Calculate a new ML hypothesis $h' = \langle \mu_1',\mu_2' \rangle$, assuming the value taken on by each hidden variable $z_{ij}$ is its expected value $E[z_{ij}]$ calculated in Step 1.
         - How?)
           - $\mu_j \leftarrow \frac{\Sigma_{i=1}^m E[z_{ij}] x_i}{\Sigma_{i=1}^m E[z_{ij}]}$
             - Desc.)
               - We are updating the estimated mean $(\mu_j)$ with the weighted sample mean $\left(\frac{\Sigma_{i=1}^m E[z_{ij}] x_i}{\Sigma_{i=1}^m E[z_{ij}]}\right)$.
      3. Replace $h$ with $h'$.


<br><br>

## 6.12.2 General Statement of EM Algorithm
### Settings.) The EM Algorithm
- The EM algorithm can be applied in many settings where we wish to estimate some set of parameters $\theta$ that describe an underlying probability distribution, given only the observed portion of the full 
data produced by this distribution.
  - i.e.) Let $X$ be observable data, $Z$ be unobservable data, and $Y$ be the full data, i.e. $Y=X\cup Z$.
    - Then, $Z$ can be treated as a random variable whose probability distribution depends on the unknown parameters $\theta$ and on the observed data $X$.
    - Also, $Y$ is a random variable because it is defined in terms of the random variable $Z$.
  - e.g.) [Above example](#6121-estimating-means-of-k-gaussians)
    - $\theta = \langle \mu_1,\mu_2 \rangle$
    - The full data : $\langle x_i, z_{i1}, z_{i2} \rangle$
      - where only $x_i$ were observed and $z_{i1}, z_{i2}$ were unobservable.
- The EM algorithm searches for the maximum likelihood hypothesis $h'$ by seeking the $h'$ that maximizes $E[\ln{P(Y|h')}]$.
  - $E[\ln{P(Y|h')}]$ is taken over the probability distribution governing $Y$, which is determined by the unknown parameters $\theta$.
    - Desc.)
      1. $P(Y|h')$ is the likelihood of the full data $Y$ given hypothesis $h'$.
      2. Maximizing $\ln{P(Y|h')}$ also maximizes $P(Y|h')$.
      3. The expected value $E[\ln{P(Y|h')}]$ is introduced because $Y$ is a random variable.
- The Probability Distribution Governing the Full Data Y
  - In general, we do not know the distribution of $Y = X\cup Z$ because it is determined by the parameters $\theta$ that we are trying to estimate.
    - Therefore, the EM algorithm uses its current hypothesis $h$ in place of the actual parameters $\theta$ to estimate the distribution governing $Y$.

<br>

### Algorithm) The EM Algorithm
- Let
  - $Q(h'|h) = E[\ln{p(Y|h')}|h, X]$ : a function of $h'$ that returns the expected likelihood under the assumption that $\theta = h$.
- Then, the EM Algorithm repeats the following two steps until convergence.
  1. Estimation Step
     - Calculate $Q(h'|h)$ using the current hypothesis $h$ and the observed data $X$ to estimate the probability distribution over $Y$.
       - $Q(h'|h) \leftarrow E[\ln{p(Y|h')}|h, X]$
  2. Maximization Step
     - Replace hypothesis $h$ by the hypothesis $h'$ that maximizes this $Q$ function.
       - $h \leftarrow argmax_{h'} Q(h'|h)$

<br>

### Props) The EM Algorithm
- When the function Q is continuous, the EM algorithm converges to a stationary point of the likelihood function $P(Y|h')$.
- When this likelihood function has a single maximum, EM will converge to this global maximum likelihood estimate for $h'$.
  - Otherwise, it is guaranteed only to converge to a local maximum just as other optimization methods such as gradient descent, line search, and conjugate gradient.

<br><br>

## 6.12.3 Derivation of the k Means Algorithm
#### Assumptions)
- There are $k$ Normal distributions sharing the same variance $\sigma^2$.
- We want to estimate $\theta = \langle \mu_1, \cdots, \mu_k \rangle$ that defines the means of these distributions.
- We are given the observed data $X=\lbrace \langle x_i \rangle \rbrace$.
- The hidden variables $Z=\lbrace \langle z_{i1}, z_{i2}, \cdots, z_{ik} \rangle \rbrace$ indicate which of the $k$ Normal distributions was used to generate $x_i$.
  - $`z_{ij}=\left\lbrace \begin{array}{ll} 1 & \textrm{if } x_i \textrm{ was created by the }j\textrm{th Normal distribution} \\ 0 & \textrm{otherwise} \end{array} \right.`$

<br>

#### Derivation)
1. The probability $p(h_i|h')$ of a single instance $y_i=\langle x_i, z_{i1}, z_{i2}, \cdots, z_{ik} \rangle$ of the full data can be written
     - $p(h_i|h') = p(x_i, z_{i1}, z_{i2}, \cdots, z_{ik}|h') = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2} \Sigma_{j=1}^k z_{ij}(x_i-\mu_j')^2}$
       - Recall that if $z_{ij_1} = 1$, then $z_{ij_2} = 0, \forall j_2 \ne j_1$
       - Therefore, this expression gives the probability distribution for xi generated by the selected Normal distribution.
2. Get the log probability for all $m$ instances $\ln{P(Y|h')}$ as follows.
   - $`\begin{array}{ll} \ln{P(Y|h')} &= \ln{\Pi_{i=1}^m p(y_i|h')} \\&= \Sigma_{i=1}^m \ln{p(y_i|h')} \\&= \Sigma_{i=1}^m \ln{\left( \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2} \Sigma_{j=1}^k z_{ij}(x_i-\mu_j')^2} \right)} \\&= \Sigma_{i=1}^m \left( \ln{\left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)} -\frac{1}{2\sigma^2} \Sigma_{j=1}^k z_{ij}(x_i-\mu_j')^2 \right) \end{array}`$
3. Get the expected value of the above, $E[\ln{P(Y|h')}]$, as follows.
   - $`\begin{array}{ll} E[\ln{P(Y|h')}] &= E \left[ \Sigma_{i=1}^m \left( \ln{\left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)} -\frac{1}{2\sigma^2} \Sigma_{j=1}^k z_{ij}(x_i-\mu_j')^2 \right) \right] \\&= \Sigma_{i=1}^m \left( \ln{\left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)} -\frac{1}{2\sigma^2} \Sigma_{j=1}^k E[z_{ij}] (x_i-\mu_j')^2 \right) \end{array}`$
     - cf.) If $f$ is a linear function of $z$, then $E[f(z)] = f(E[z])$ 
4. Now, our likelihood goes as follows.
   - $Q(h'|h)=\Sigma_{i=1}^m \left( \ln{\left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)} -\frac{1}{2\sigma^2} \Sigma_{j=1}^k E[z_{ij}] (x_i-\mu_j')^2 \right)$
     - where $h'=\langle \mu_1', \cdots, \mu_k' \rangle$
       - and $E[z_{ij}]$ is calculated based on the current hypothesis $h$ and the observed data $X$.
         - Recall that $E[z_{ij}] = \frac{e^{-\frac{1}{2\sigma^2}(x_i-\mu_j)^2}}{\Sigma_{n \in \lbrace 1, 2\rbrace} e^{-\frac{1}{2\sigma^2}(x_i-\mu_n)^2}}$. (Refer to the [above](#example-case-k2).)
5. Getting the maximum likelihood.
   - $`\begin{array}{ll} argmax_{h'} Q(h'|h) &= argmax_{h'} \Sigma_{i=1}^m \left( \ln{\left( \frac{1}{\sqrt{2\pi\sigma^2}}\right)} -\frac{1}{2\sigma^2} \Sigma_{j=1}^k E[z_{ij}] (x_i-\mu_j')^2 \right) \\&= argmin_{h'} \Sigma_{i=1}^m \Sigma_{j=1}^k E[z_{ij}](x_i-\mu_j')^2 \end{array}`$
     - The maximum likelihood estimate $h_{ML}$ minimizes the quantity above by setting each $\mu_j'$ to the weighted sample mean $\mu_j \leftarrow \frac{\Sigma_{i=1}^m E[z_{ij}] x_i}{\Sigma_{i=1}^m E[z_{ij}]}$.



<br>

* [Back to Machine Learning Tom Mitchell Main](../../main.md)