* [Back to Machine Learning Tom Mitchell Main](../../main.md)

# 4.4 Perceptrons

#### Concept) Perceptron
- Desc.)
  - Takes a vector of real-valued inputs
  - Calculates a linear combination of these inputs
  - Outputs a 1 if the result is greater than some threshold and -1 otherwise.
- Notation)
  - Inputs : $x_1, x_2, ..., x_n$
  - Outputs : $`o(x_1, x_2, ..., x_n) = \left\lbrace \begin{array}{cl} 1 & if \space w_0+w_1x_1+w_2x_2+\cdots+w_nx_n \gt 0 \\ -1 & otherwise \end{array} \right.`$
    - where each $w_i$ is a real-valued constant or weight
      - $w_i$ determines the contribution of input $x_i$
    - Put $\Sigma_{i=0}^n w_ix_i = w\cdot x \gt 0$.
    - Also, $o(x) = sgn(w\cdot x)$
      - where $`sgn(y)=\left\lbrace \begin{array}{cl} 1 & if \space y \gt 0 \\ -1 & otherwise \end{array} \right.`$

![](images/001.png)

#### Concept) Learning a perceptron
- Involves choosing values for the weights $w_0, \dots, w_n$.
- Thus, the candidate hypotheses' space $H$ is the set of all possible real-valued weight vectors.
  - i.e.) $H = \lbrace w | w \in \mathbb{R}^{n+1} \rbrace$

<br><br>

## 4.4.1 Representational Power of Perceptrons
#### Concept) Geometrical View of a Perceptron
<table>
<tr><td><img src="images/002.png"></td><td>We can view the perceptron as representing a hyperplane decision surface in the n-dimensional space of instances (i.e., points). <br><br> <img src="images/003.png">  </td></tr>
</table>

- Above cases are **linearly separable**.
- Some operations like XOR are not separable.   
  ![](images/004.png)

<br>

#### Concept) Primitive Boolean Functions Representation
Assume there are two variables $x_1$ and $x_2$.    
Recall that $x_i \in \lbrace 0,1 \rbrace$
|Function|Perceptron Representation|
|:------:|:------------------------|
|AND|$w_0=-0.8, \space w_1=w_2=0.5$|
|OR|$w_0=-0.3, \space w_1=w_2=0.5$|
|NAND|$w_0=0.8, \space w_1=w_2=-0.5$|
|NOR|$w_0=0.3, \space w_1=w_2=-0.5$|
- Prop.)
  - Every boolean function can be represented by some network of interconnected units based on these primitives.
  - How?)
    - Represent a boolean function in disjunctive normal form.
      - i.e.) as the disjunction(OR) of a set of conjunctions (ANDs) of the inputs and their negations.
      - cf.) Negating AND Perceptron
        - Change the sign of the corresponding input weight.
        - Compare AND and NAND above.

<br><br>

## 4.4.2 The Perceptron Training Rule
#### Concept) Learn the Weights for a Single Perceptron
- Goal)
  - Determine a weight vector that causes the perceptron to produce the correct $\plusmn 1$ output for each of the given training examples.
- Algorithms)
  - Algorithms Considered)
    1. The Perceptron Rule
       - $w_i \leftarrow w_i + \Delta w_i$
    2. The Delta Rule
       - $\Delta w_i = \eta(t-o)x_i$
         - $t$ : the target output for the current training examples
         - $o$ : the output generated by the perceptron
         - $\eta$ : the learning rate
  - Procedure)
    - Start with random weights.
    - Iteratively apply the perceptron to each training example.
    - Modify the perceptron weights whenever it misclassifies an example.
    - Repeat until the perceptron classifies all training examples correctly.
  - How updating weights works?)
    - Case 1) Training example is correctly classified by the perceptron.
      - Then, $t=o$. Thus, $\Delta w_i = 0$. No update.
    - Case 2) $t=1$ and $o=-1$.
      - Then, the weights must be altered to increase the value of $w \cdot x$.
        - If $x_i \gt 0$, then increase $w_i$.
        - Else if $x_i \lt 0$, decrease $w_i$.
      - e.g.) $x_i=0.8, \eta=0.1, t=1, o=-1$
        - Then, $\Delta w_i = \eta(t-o)x_i = 0.1\cdot(1-(-1))\cdot0.8 = 0.16$
    - Case 3) $t=-1$ and $o=1$.
      - Then, the weights must be altered to decrease the value of $w \cdot x$.
        - If $x_i \gt 0$, then decrease $w_i$.
        - Else if $x_i \lt 0$, increase $w_i$.
  - Convergence Condition)
    1. The training examples are linearly separable
    2. A sufficiently small $\eta$ is used.
- Props.)
  - Guaranteed to converge to somewhat different acceptable hypotheses, under somewhat different conditions. 
  - Important to ANNs because they provide the basis for learning networks of many units.


<br>

* [Back to Machine Learning Tom Mitchell Main](../../main.md)