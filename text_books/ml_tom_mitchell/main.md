[Back to AI Main](../../README.md)

<br>

# Machine Learing
### Tom M. Mitchell

<br>

## 1. Introduction
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|1.1|[Well-Posed Learning Problems](./ch01/01/note.md)|Learning|
|1.2|[Designing a Learning System](./ch01/02/note.md)|Experience, Target Function, Nonoperational Definition, Operational Description, Representation of a Target Function, Training Example, Estimation, Least Mean Square(LMS), Performance System, The Critic, The Generalizer, The Experiment Generator|

<br>

## 2. Concept Learning and The General-To-Specific Ordering
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|2.2|[A Concept Learning Task](./ch02/02/note.md)|Concept Learning, Positive Example, Negative Example, Hypothesis, Inductive Learning Hypothesis|
|2.3|[Concept Learning as Search](./ch02/03/note.md)|Hypothesis Space, (Syntactically/Semantically) Distinct Hypotheses, General-To-Specific Ordering ($\geq_g$)|
|2.4|[Find-S: Finding a Maximally Specific Hypothesis](./ch02/04/note.md)|Find-S Algorithm|
|2.5|[Version Spaces and the Candidate-Elimination Algorithm](./ch02/05/note.md)|Consistency, Version Space, List-Then-Elimination Algorithm, Candidate-Elimination Algorithm, General Boundary, Specific Boundary, Version Space Representation Theorem|
|2.6|[Remarks on Version Spaces and Candidate-Elimination](ch02/06/note.md)|Query|
|2.7|[Inductive Bias](ch02/07/note.md)|Power Set, Unbiased Learner, Inductive Bias, Inductive Inference Systems|

<br>

## 3. Decision Tree Learning
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|3.2|[Decision Tree Representation](./ch03/02/note.md)|Node, Branch|
|3.3|[Appropriate Problems for Decision Tree Learning](./ch03/03/note.md)||
|3.4|[The Basic Decision Tree Learning Algorithm](./ch03/04/note.md)|Entropy, Information Gain, ID3|
|3.5|[Hypothesis Space Search in Decision Tree Learning](./ch03/05/note.md)|ID3|
|3.6|[Inductive Bias in Decision Tree Learning](./ch03/06/note.md)|BFS-ID3, Preference Bias(Search Bias), Restriction Bias(Language Bias)|
|3.7|[Issues in Decision Tree Learning](./ch03/07/note.md)|Overfit, Training and Validation Approach, Reduced-Error Pruning, Rule Post-Pruning, Rule Antecedent(precondition), Rule Consequent(postcondition), Pessimistic Estimate, Split Information, Gain Ratio|

<br>

## 4. Artificial Neural Networks (ANNs)
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|4.2|[Neural Network Representations](./ch04/02/note.md)|ALVINN|
|4.3|[Appropriate Problems for Neural Network Learning](./ch04/03/note.md)||
|4.4|[Perceptrons](./ch04/04/note.md)|Perceptron Training Rule, Threshold, Delta Rule, Gradient Descent, Stochastic Gradient Descent (Incremental Gradient Descent)|
|4.5|[Multilayer Networks and the Backpropagation Algorithm](./ch04/05/note.md)|Sigmoid Unit (logistic function), Momentum|
|4.6|[Remarks on the Backpropagation Algorithm](./ch04/06/note.md)|Generalization Accuracy, Weight Decay, k-fold Cross-Validation Approach|





<br><br>

[Back to AI Main](../../README.md)