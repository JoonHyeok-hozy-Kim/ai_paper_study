[Back to AI Main](../../README.md)

<br>

# Elements of Information Theory (2006)
*THOMAS M. COVER, JOY A. THOMAS*


## 2. Entropy, Relative Entropy, and Mutual Information
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|2.1|[Entropy](ch02/01/note.md)|- $`H(p)`$|
|2.2|[Joint Entropy and Conditional Entropy](ch02/02/note.md)|- Chain Rule : $`H(X,Y) = H(X) + H(Y\|X)`$|
|2.3|[Relative Entropy and Mutual Information](ch02/03/note.md)|- Relative Entropy (Kullbackâ€“Leibler Distance) : $`D(p\|\|q)`$ <br> - Mutual Information : $`I(X;Y)`$|
|2.4|[Relationship between Entropy and Mutual Information](ch02/04/note.md)|- Self Information|
|2.5|[Chain Rules for Entropy, Relative Entropy, and Mutual Information](ch02/05/note.md)|- Chain Rule for Entropy : $`\displaystyle H(X_1, X_2, \cdots, X_n) = \sum_{i=1}^n H(X_i\|X_1, \cdots, X_{i-1})`$ <br>- Conditional Mutual Information <br> - Chain Rule for Information : $`\displaystyle I(X_1, X_2, \cdots, X_n; Y) = \sum_{i=1}^n I(X_i; Y\|X_1, X_2, \cdots, X_{i-1})`$ <br> - Chain Rule for Relative Entropy : $`D(p(x,y) \|\| q(x,y)) = D(p(x) \|\| q(x)) + D(p(y\|x) \|\| q(y\|x))`$|
|2.6|[Jensen's Inequality and Its Consequences](ch02/06/note.md)|- Convexity / Concavity <br> - Jensen's Inequality <br> - Information Inequality <br> - Non-negativity of Mutual Information / Conditional Relative Entropy / Conditional Mutual Information <br> - Independence Bound on Entropy|
|2.7|[Log Sum Inequality and its Applications](ch02/07/note.md)|- Log Sum Inequality <br> - Convexity of Relative Entropy <br> - Concavity of Entropy <br> - Convexity / Concavity of Mutual Information|





<br><br>

[Back to AI Main](../../README.md)