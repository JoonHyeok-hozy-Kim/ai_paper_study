[Back to AI Main](../../README.md)

<br>

# Elements of Information Theory (2006)
*THOMAS M. COVER, JOY A. THOMAS*


## 2. Entropy, Relative Entropy, and Mutual Information
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|2.1|[Entropy](ch02/01/note.md)|- |
|2.2|[Joint Entropy and Conditional Entropy](ch02/02/note.md)|- Chain Rule|
|2.3|[Relative Entropy and Mutual Information](ch02/03/note.md)|- Relative Entropy (Kullbackâ€“Leibler Distance) $`D(p\|\|q)`$ <br> - Mutual Information $`I(X;Y)`$|
|2.4|[Relationship between Entropy and Mutual Information](ch02/04/note.md)|- Self Information|
|2.5|[Chain Rules for Entropy, Relative Entropy, and Mutual Information](ch02/05/note.md)|- Chain Rule for Entropy <br>- Conditional Mutual Information <br> - Chain Rule for Information <br> - Chain Rule for Relative Entropy|
|2.6|[Jensen's Inequality and Its Consequences](ch02/06/note.md)|- Convexity / Concavity <br> - Jensen's Inequality <br> - Information Inequality <br> - Non-negativity of Mutual Information / Conditional Relative Entropy / Conditional Mutual Information <br> - Independence Bound on Entropy|
|2.7|[Log Sum Inequality and its Applications](ch02/07/note.md)|- Log Sum Inequality <br> - Convexity of Relative Entropy <br> - Concavity of Entropy <br> - Convexity / Concavity of Mutual Information|





<br><br>

[Back to AI Main](../../README.md)