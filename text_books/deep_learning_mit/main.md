[Back to AI Main](../../README.md)

<br>

# Deep Learning (2017, MIT)
*Ian Goodfellow, Yoshua Bengio, Aaron Courville*


## 2. Linear Algebra
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|2.1|[Scalars, Vectors, Matrices, and Tensors](ch02/01/note.md)|- Transpose, Broadcasting|
|2.2|[Multiplying Matrices and Vectors](ch02/02/note.md)|- Hadamard Product <br> - Associative, Commutative <br> - System of Linear Equations|
|2.3|[Identity and Inverse Matrices](ch02/03/note.md)|- |
|2.4|[Linear Dependence and Span](ch02/04/note.md)|- Column Space, Singularity|
|2.5|[Norms](ch02/05/note.md)|- $L^p$ Norm, Max Norm, Frobenius Norm|
|2.6|[Special Kinds of Matrices and Vectors](ch02/06/note.md)|- Diagonal Matrix, Symmetric Matrix <br> - Unit Vector <br> - Orthogonality, Orthonormality, Orthogonal Matrix|
|2.7|[Eigendecomposition](ch02/07/note.md)|- Eigenvector, Eigenvalue <br> - Positive Definite, Negative Definite|
|2.8|[Singular Value Decomposition](ch02/08/note.md)|- $`A = UDV^\top`$|
|2.9|[The Moore-Penrose Pseudoinverse](ch02/09/note.md)|- $`\displaystyle A^{+} \equiv \lim_{\alpha\rightarrow 0} \left(A^\top A + \alpha I \right)^{-1} A^\top`$|
|2.10|[The Trace Operator](ch02/10/note.md)|- |
|2.11|[The Determinant](ch02/11/note.md)|- |
|2.12|[Principal Components Analysis](ch02/12/note.md)|- |


## 3. Probability and Information Theory
|No.|Chapter|Keywords|
|:-:|:------|:-------|
|3.2|[Random Variables](ch03/02/note.md)|- |
|3.3|[Probability Distributions](ch03/03/note.md)|- Probabilities Mass Function (PMF), Uniform Distribution <br> - Joint Probability Distribution <br>- Probabilities Density Function (PDF)|
|3.4|[Marginal Probability](ch03/04/note.md)|- |
|3.5|[Conditional Probability](ch03/05/note.md)|- The Chain Rule of Conditional Probabilities|
|3.7|[Independence and Conditional Independence](ch03/07/note.md)|- |





<br><br>

[Back to AI Main](../../README.md)