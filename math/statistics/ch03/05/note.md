* [Back to Statistics Main](../../main.md)

# 3.5 Limit Theorems 
### Theorem) Chebyshev's Theorem
![](images/001.png)   
![](images/002.png)   

Chebyshev’s inequality states that the probability that a random variable $X$ differs from its mean by at least K standard deviations is less than or equal to $\frac{1}{K^2} (K\ge2)$.

<br>

#### Props.) Chebyshev's Theorem
- Another form
  ![](images/003.png)
  - Why?)   
    ![](images/004.png)
- The Chebyshev's Theorem can be used **regardless of the distribution** of the underlying population, as long as we know its mean and variance.

#### Example) Chebyshev's Theorem
For any data set (regardless of the shape of the distribution), at least $(1−\frac{1}{K^2})100\%$ of observations will lie within $K(\ge1)$ standard deviations of the mean. 
- e.g.) 
  - At least $(1−\frac{1}{K^2})100\%$ = 75% of the data will fall in the interval $(\bar{x}−2s, \bar{x}+2s)$.
  - At least 88.9% of the observations will lie within three standard deviations of the mean.




<br><br>

### [Exercises](./exercises.md)

<br><br>

* [Back to Statistics Main](../../main.md)