[Back to Linear Algebra main](../../main.md)

# 14. Orthogonal Vectors and Subspaces / 15. Projections onto Subspaces
### Concept) Orthogonality
- Vectors)
  - Def.) $`\mathbf{u}^\top \mathbf{v} = 0`$
- Subspaces)
  - Def.) Subspaces $`U, V`$ are orthogonal iff. $`\forall \mathbf{u}\in U, \forall\mathbf{v}\in V, \mathbf{u}^\top \mathbf{v} = 0`$

<br>

### Concept) Orthogonality in Vector Space
- Props.)
  - [Null Space and row space](0910.md#concept-four-fundamental-subspaces) are orthogonal complements in $`\mathbb{R}^n`$
  - $`\text{Null}(A^\top)`$ and [column space](0910.md#concept-four-fundamental-subspaces) are orthogonal complements in $`\mathbb{R}^n`$

<br>

### Prop.) Transpose Cases
- Props.)
  - $`\text{Null}(A^\top A) = \text{Null}(A)`$
  - $`\text{Rank}(A^\top A) = \text{Rank}(A)`$

<br>

### Concept) Projection on R1
- Def.)
  - Let $`a, b \in\mathbb{R}^n`$.
  - Then the projection of $`b`$ on $`a`$ is $`\displaystyle \text{Proj}_a(b) = \left(\frac{a^\top b}{a^\top a}\right) a`$
- Derivation)
  - Put $`p = \text{Proj}_a(b)`$.
  - Then $`\exists x\in\mathbb{R}`$ s.t. $`p = xa`$.
  - Using the property of the [orthogonality](#concept-orthogonality) we have   
    $`\begin{aligned}
        a^\top (b-p) &= a^\top (b-xa) \\
        &= a^\top b - x a^\top a = 0
    \end{aligned}`$
  - Thus, we have $`\displaystyle x = \frac{a^\top b}{a^\top a}`$.
  - Hence, $`\displaystyle \text{Proj}_a(b) = p = xa = a\left(\frac{a^\top b}{a^\top a}\right)`$

<br>

### Concept) Projection Matrix
- Def.)
  - A matrix $`P\in\mathbb{R}^{n\times n}`$ s.t. $`\text{Proj}_a(b) = Pb, \; \exists a,b\in\mathbb{R}^n`$
- Prop.)
  - $`\displaystyle P = \frac{aa^\top }{a^\top a}`$ where $`a\in\mathbb{R}^{n\times 1}`$
    - Pf.)
      - Recall that $`\displaystyle \text{Proj}_a(b) =  a\left(\frac{a^\top b}{a^\top a}\right) = \left(\frac{aa^\top }{a^\top a}\right) b`$
      - Here, $`aa^\top \in\mathbb{R}^{n\times n}, a^\top a \in\mathbb{R}`$
  - $`C(P) = c a, \forall c\in\mathbb{R}`$
    - Why?)
      - $`\displaystyle P = \left(\frac{1}{a^\top a}\right)aa^\top = \left(\frac{1}{a^\top a}\right)\begin{bmatrix}
        a_1 a & a_2 a & \cdots & a_n a
      \end{bmatrix}`$
  - $`\text{Rank}(P) = 1`$
    - Why?)
      - $`\displaystyle P = \left(\frac{1}{a^\top a}\right)aa^\top = \left(\frac{1}{a^\top a}\right)\begin{bmatrix}
        a_1 a \\ a_2 a \\ \vdots \\ a_n a
      \end{bmatrix}`$.
      - Thus, by performing the Gaussian Elimination we get the [RREF](0708.md#concept-reduced-row-echelon-form-rref) of P as $`\begin{bmatrix}
        a \\ 0 \\ \vdots \\ 0
      \end{bmatrix}`$
  - $`P^2 = P`$
    - Why?)
      - Projection of $`p`$ on $`a`$ is $`p`$ itself.

<br>

### Concept) Projection on Rn
- Def.)
  - Consider a hyperplane in $`\mathbb{R}^n`$ and a vector $`b\in\mathbb{R}^n`$.
  - Then, the projection of $`b`$ on the hyperplane is given by   
   $`p = A\hat{x}`$
    - where
      - $`A=\begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}`$ for bases $`a_1, a_2, \cdots, a_n\in\mathbb{R}^n`$ of the hyperplane
      - $`\hat{x} = \begin{bmatrix}
        \hat{x_1} \\ \hat{x_2} \\ \vdots \\ \hat{x_n}
      \end{bmatrix}\in\mathbb{R}^n`$
- Derivation)
  - Consider the hyperplane can also be seen as $`C(A)`$ where $`A=\begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}`$ for bases $`a_1, a_2, \cdots, a_n\in\mathbb{R}^n`$.
    - Why?)
      - Each vector in the hyperplane are the linear combinations of the bases.
  - Now put $`b\in\mathbb{R}^n`$ and we want to get $`p\in C(A)`$ which is the projection of $`b`$ on the plane.
  - We can denote $`p = \hat{x_1}a_1 + \hat{x_2}a_2 + \cdots + \hat{x_n}a_n = A\hat{x}, \exists \hat{x_1}, \cdots, \; \hat{x_n} \in\mathbb{R}`$
- Deriving $`\hat{x}`$)
  - $`\hat{x} = (A^\top A)^{-1}(A^\top b)`$
    - How?)
      - By the orthogonality, $`p^\top (b-p) = (A\hat{x})^\top (b-A\hat{x}) = 0`$.
      - Thus, $`\hat{x}^\top A^\top (b-A\hat{x}) = \hat{x}^\top (A^\top b -  A^\top A\hat{x}) = 0`$
      - Hence, $`A^\top b -  A^\top A\hat{x} = 0 \Rightarrow \hat{x} = (A^\top A)^{-1}(A^\top b)`$
    - Application)
      - Consider the case when $`Ax=b`$ does not have solution.
        - Then we can use $`A\hat{x} = p`$ instead.
- Deriving the projection matrix $`P`$)
  - $`P = A(A^\top A)^{-1}A^\top`$
    - How?)
      - Recall that $`P`$ was a matrix s.t. $`p = Pb`$.
      - Then $`p = A\hat{x} = A(A^\top A)^{-1}(A^\top b) = (A(A^\top A)^{-1}A^\top) b`$
      - Thus, $`P = A(A^\top A)^{-1}A^\top`$
    - Cf.)
      - Recall that $`A`$ was not a square matrix.
      - If it was the square matrix, then $`P=I_n \Leftrightarrow b = p`$
    - Cf.)
      - $`P^2 = P`$ still holds.

<br>

[Back to Linear Algebra main](../../main.md)